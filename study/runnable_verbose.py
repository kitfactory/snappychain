"""
Custom OpenAI Chat implementation with verbose output using RunnableLambda
RunnableLambdaã‚’ä½¿ç”¨ã—ãŸè©³ç´°å‡ºåŠ›ä»˜ãã®ã‚«ã‚¹ã‚¿ãƒ OpenAIãƒãƒ£ãƒƒãƒˆå®Ÿè£…
"""

from langchain_core.runnables import RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
import time
import os
import json
import sys
import io

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
# Log file configuration
log_file_path = "study/verbose_output.log"
log_file = open(log_file_path, "w", encoding="utf-8")

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è­¦å‘Šã‚’è¡¨ç¤ºï¼‰
# Get API key from environment variable (show warning if not set)
api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    print("è­¦å‘Š: OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    print("Warning: OPENAI_API_KEY is not set.")
    log_file.write("è­¦å‘Š: OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n")
    log_file.write("Warning: OPENAI_API_KEY is not set.\n")

# ã‚«ã‚¹ã‚¿ãƒ ã®ãƒ‡ãƒãƒƒã‚°é–¢æ•°
# Custom debug function
def debug_print(prefix, data, verbose=False):
    """
    ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›ã™ã‚‹é–¢æ•°
    Function to output debug information
    
    Args:
        prefix (str): å‡ºåŠ›ã®æ¥é ­è¾ / Output prefix
        data (any): å‡ºåŠ›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ / Data to output
        verbose (bool): è©³ç´°å‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹ / Whether in verbose mode
    """
    if verbose:
        separator = f"\n{'='*20} {prefix} {'='*20}"
        print(separator)
        log_file.write(f"{separator}\n")
        
        if isinstance(data, dict) or isinstance(data, list):
            json_data = json.dumps(data, ensure_ascii=False, indent=2)
            print(json_data)
            log_file.write(f"{json_data}\n")
        else:
            print(data)
            log_file.write(f"{data}\n")
        
        end_separator = f"{'='*50}\n"
        print(end_separator)
        log_file.write(f"{end_separator}\n")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã—ã¦ç¢ºå®Ÿã«æ›¸ãè¾¼ã‚€
        # Flush the file to ensure writing
        log_file.flush()

# ã‚«ã‚¹ã‚¿ãƒ ã®OpenAIãƒãƒ£ãƒƒãƒˆé–¢æ•°
# Custom OpenAI chat function
def custom_openai_chat(data, model_name="gpt-4o-mini", temperature=0.7, verbose=False):
    """
    OpenAIã®ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°
    Custom function to generate responses using OpenAI's chat model
    
    Args:
        data (dict): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ / Input data
        model_name (str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å / Model name to use
        temperature (float): æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ / Temperature parameter
        verbose (bool): è©³ç´°å‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹ / Whether in verbose mode
        
    Returns:
        str: ç”Ÿæˆã•ã‚ŒãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ / Generated response
    """
    # verboseãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆconfigã«æ ¼ç´ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    # Get verbose parameter (if stored in config)
    if hasattr(data, "get") and callable(data.get):
        config = data.get("config", {})
        if isinstance(config, dict):
            verbose = config.get("verbose", verbose)
    
    debug_print("å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ / Input Data", data, verbose)
    
    # å‡¦ç†é–‹å§‹æ™‚é–“ã‚’è¨˜éŒ²
    # Record processing start time
    start_time = time.time()
    
    # OpenAIã®ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    # Initialize OpenAI chat model
    llm = ChatOpenAI(model_name=model_name, temperature=temperature)
    debug_print("ä½¿ç”¨ãƒ¢ãƒ‡ãƒ« / Model Used", f"{model_name} (temperature={temperature})", verbose)
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
    # Generate response
    response = llm.invoke(data)
    
    # å‡¦ç†æ™‚é–“ã‚’è¨ˆç®—
    # Calculate processing time
    processing_time = time.time() - start_time
    
    debug_print("ç”Ÿæˆã•ã‚ŒãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ / Generated Response", response.content, verbose)
    debug_print("å‡¦ç†æ™‚é–“ / Processing Time", f"{processing_time:.2f}ç§’ / seconds", verbose)
    
    return response.content

# RunnableLambdaã§ãƒ©ãƒƒãƒ—ã—ãŸã‚«ã‚¹ã‚¿ãƒ é–¢æ•°
# Custom function wrapped in RunnableLambda
def create_custom_chain(model_name="gpt-4o-mini", temperature=0.7):
    """
    ã‚«ã‚¹ã‚¿ãƒ ãƒã‚§ã‚¤ãƒ³ã‚’ä½œæˆã™ã‚‹é–¢æ•°
    Function to create a custom chain
    
    Args:
        model_name (str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å / Model name to use
        temperature (float): æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ / Temperature parameter
        
    Returns:
        Chain: ä½œæˆã•ã‚ŒãŸãƒã‚§ã‚¤ãƒ³ / Created chain
    """
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®šç¾©
    # Define system prompt and user prompt
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template("ã‚ãªãŸã¯çŸ¥è­˜è±Šå¯Œãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"),
        HumanMessagePromptTemplate.from_template("{question}")
    ])
    
    # å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼
    # Output parser
    output_parser = StrOutputParser()
    
    # å…¥åŠ›å‡¦ç†ç”¨ã®RunnableLambda
    # RunnableLambda for input processing
    def input_processor_func(*args, **kwargs):
        # verboseãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        # Get verbose parameter
        verbose = kwargs.get("verbose", False)
        
        # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        # Debug output
        debug_print("å…¥åŠ›ãƒ—ãƒ­ã‚»ãƒƒã‚µå¼•æ•° / Input Processor Args", args, verbose)
        debug_print("å…¥åŠ›ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•° / Input Processor Kwargs", kwargs, verbose)
        
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        # Get input data
        input_data = args[0] if args else kwargs.get("input", {})
        
        # configã«æ ¼ç´
        # Store in config
        config = {"verbose": verbose}
        
        # configã‚’è¿½åŠ 
        # Add config
        if isinstance(input_data, dict):
            input_with_config = {**input_data, "config": config}
        else:
            input_with_config = input_data
        
        return input_with_config

    input_processor = RunnableLambda(input_processor_func)

    # å‡ºåŠ›å‡¦ç†ç”¨ã®RunnableLambda
    # RunnableLambda for output processing
    def output_processor_func(output, **kwargs):
        # verboseãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        # Get verbose parameter
        verbose = kwargs.get("verbose", False)
        
        # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        # Debug output
        debug_print("å‡ºåŠ›ãƒ—ãƒ­ã‚»ãƒƒã‚µçµæœ / Output Processor Result", output, verbose)
        
        # çµæœã‚’è¿”ã™
        # Return result
        return output

    output_processor = RunnableLambda(output_processor_func)

    # ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†ç”¨RunnableLambda
    # RunnableLambda for main processing
    def main_processor_func(data, **kwargs):
        # verboseãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        # Get verbose parameter
        verbose = kwargs.get("verbose", False)
        
        # ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰configã‚’å–å¾—
        # Get config from data
        config = data.get("config", {}) if isinstance(data, dict) else {}
        
        # configã‹ã‚‰verboseã‚’å–å¾—ï¼ˆå„ªå…ˆï¼‰
        # Get verbose from config (priority)
        verbose = config.get("verbose", verbose)
        
        # ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°ã‚’å‘¼ã³å‡ºã—
        # Call custom function
        return custom_openai_chat(data, model_name=model_name, temperature=temperature, verbose=verbose)

    main_processor = RunnableLambda(main_processor_func)
    
    # ãƒã‚§ã‚¤ãƒ³ã‚’æ§‹ç¯‰
    # Build the chain
    chain = input_processor | prompt | main_processor | output_parser | output_processor
    
    return chain

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
# Main execution part
if __name__ == "__main__":
    try:
        # ãƒã‚§ã‚¤ãƒ³ã‚’ä½œæˆ
        # Create chain
        chain = create_custom_chain()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›
        # User input
        user_input = {"question": "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"}
        
        print("\nğŸ” ãƒã‚§ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™... / Running the chain...\n")
        log_file.write("\nğŸ” ãƒã‚§ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™... / Running the chain...\n\n")
        
        # ãƒã‚§ã‚¤ãƒ³ã‚’å®Ÿè¡Œï¼ˆverboseãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã™ï¼‰
        # Run the chain (pass verbose parameter)
        response = chain.invoke(user_input, verbose=True)
        
        print("\nğŸ“ æœ€çµ‚çµæœ / Final Result:")
        print(f"\n{response}\n")
        
        log_file.write("\nğŸ“ æœ€çµ‚çµæœ / Final Result:\n")
        log_file.write(f"\n{response}\n\n")
        
        print(f"\nâœ… è©³ç´°ãªãƒ­ã‚°ã¯ {log_file_path} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        print(f"\nâœ… Detailed logs have been saved to {log_file_path}.")
    
    except Exception as e:
        error_message = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        print(f"\nâŒ {error_message}")
        log_file.write(f"\nâŒ {error_message}\n")
        import traceback
        traceback_str = traceback.format_exc()
        print(traceback_str)
        log_file.write(f"{traceback_str}\n")
    
    finally:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‰ã˜ã‚‹
        # Close the file
        log_file.close()
