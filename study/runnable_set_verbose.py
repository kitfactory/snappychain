"""
Custom OpenAI Chat implementation with verbose output using set_verbose
set_verboseã‚’ä½¿ç”¨ã—ãŸè©³ç´°å‡ºåŠ›ä»˜ãã®ã‚«ã‚¹ã‚¿ãƒ OpenAIãƒãƒ£ãƒƒãƒˆå®Ÿè£…
"""

from langchain_core.runnables import RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.globals import set_verbose, get_verbose
import time
import os
import json
import sys
import io

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
# Log file configuration
log_file_path = "study/set_verbose_output.log"
log_file = open(log_file_path, "w", encoding="utf-8")

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è­¦å‘Šã‚’è¡¨ç¤ºï¼‰
# Get API key from environment variable (show warning if not set)
api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    print("è­¦å‘Š: OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    print("Warning: OPENAI_API_KEY is not set.")
    log_file.write("è­¦å‘Š: OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n")
    log_file.write("Warning: OPENAI_API_KEY is not set.\n")

# ã‚«ã‚¹ã‚¿ãƒ ã®ãƒ‡ãƒãƒƒã‚°é–¢æ•°
# Custom debug function
def debug_print(prefix, data):
    """
    ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›ã™ã‚‹é–¢æ•°ï¼ˆset_verboseã®çŠ¶æ…‹ã«å¿œã˜ã¦åˆ‡ã‚Šæ›¿ã‚ã‚‹ï¼‰
    Function to output debug information (toggles based on set_verbose state)
    
    Args:
        prefix (str): å‡ºåŠ›ã®æ¥é ­è¾ / Output prefix
        data (any): å‡ºåŠ›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ / Data to output
    """
    # get_verbose()ã‚’ä½¿ç”¨ã—ã¦ç¾åœ¨ã®verboseè¨­å®šã‚’å–å¾—
    # Get current verbose setting using get_verbose()
    if get_verbose():
        separator = f"\n{'='*20} {prefix} {'='*20}"
        print(separator)
        log_file.write(f"{separator}\n")
        
        if isinstance(data, dict) or isinstance(data, list):
            json_data = json.dumps(data, ensure_ascii=False, indent=2)
            print(json_data)
            log_file.write(f"{json_data}\n")
        else:
            print(data)
            log_file.write(f"{data}\n")
        
        end_separator = f"{'='*50}\n"
        print(end_separator)
        log_file.write(f"{end_separator}\n")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã—ã¦ç¢ºå®Ÿã«æ›¸ãè¾¼ã‚€
        # Flush the file to ensure writing
        log_file.flush()

# ã‚«ã‚¹ã‚¿ãƒ ã®OpenAIãƒãƒ£ãƒƒãƒˆé–¢æ•°
# Custom OpenAI chat function
def custom_openai_chat(data, model_name="gpt-4o-mini", temperature=0.7):
    """
    OpenAIã®ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°
    Custom function to generate responses using OpenAI's chat model
    
    Args:
        data (dict): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ / Input data
        model_name (str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å / Model name to use
        temperature (float): æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ / Temperature parameter
        
    Returns:
        str: ç”Ÿæˆã•ã‚ŒãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ / Generated response
    """
    debug_print("å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ / Input Data", data)
    
    # å‡¦ç†é–‹å§‹æ™‚é–“ã‚’è¨˜éŒ²
    # Record processing start time
    start_time = time.time()
    
    # OpenAIã®ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    # Initialize OpenAI chat model
    llm = ChatOpenAI(model_name=model_name, temperature=temperature)
    debug_print("ä½¿ç”¨ãƒ¢ãƒ‡ãƒ« / Model Used", f"{model_name} (temperature={temperature})")
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
    # Generate response
    response = llm.invoke(data)
    
    # å‡¦ç†æ™‚é–“ã‚’è¨ˆç®—
    # Calculate processing time
    processing_time = time.time() - start_time
    
    debug_print("ç”Ÿæˆã•ã‚ŒãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ / Generated Response", response.content)
    debug_print("å‡¦ç†æ™‚é–“ / Processing Time", f"{processing_time:.2f}ç§’ / seconds")
    
    return response.content

# RunnableLambdaã§ãƒ©ãƒƒãƒ—ã—ãŸã‚«ã‚¹ã‚¿ãƒ é–¢æ•°
# Custom function wrapped in RunnableLambda
def create_custom_chain(model_name="gpt-4o-mini", temperature=0.7):
    """
    ã‚«ã‚¹ã‚¿ãƒ ãƒã‚§ã‚¤ãƒ³ã‚’ä½œæˆã™ã‚‹é–¢æ•°
    Function to create a custom chain
    
    Args:
        model_name (str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å / Model name to use
        temperature (float): æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ / Temperature parameter
        
    Returns:
        Chain: ä½œæˆã•ã‚ŒãŸãƒã‚§ã‚¤ãƒ³ / Created chain
    """
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®šç¾©
    # Define system prompt and user prompt
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template("ã‚ãªãŸã¯çŸ¥è­˜è±Šå¯Œãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"),
        HumanMessagePromptTemplate.from_template("{question}")
    ])
    
    # å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼
    # Output parser
    output_parser = StrOutputParser()
    
    # å…¥åŠ›å‡¦ç†ç”¨ã®RunnableLambda
    # RunnableLambda for input processing
    def input_processor_func(data):
        debug_print("å…¥åŠ›ãƒ—ãƒ­ã‚»ãƒƒã‚µ / Input Processor", data)
        return data
    
    input_processor = RunnableLambda(input_processor_func)
    
    # å‡ºåŠ›å‡¦ç†ç”¨ã®RunnableLambda
    # RunnableLambda for output processing
    def output_processor_func(output):
        debug_print("å‡ºåŠ›ãƒ—ãƒ­ã‚»ãƒƒã‚µçµæœ / Output Processor Result", output)
        return output
    
    output_processor = RunnableLambda(output_processor_func)
    
    # ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†ç”¨RunnableLambda
    # RunnableLambda for main processing
    def main_processor_func(data):
        return custom_openai_chat(data, model_name=model_name, temperature=temperature)
    
    main_processor = RunnableLambda(main_processor_func)
    
    # ãƒã‚§ã‚¤ãƒ³ã‚’æ§‹ç¯‰
    # Build the chain
    chain = input_processor | prompt | main_processor | output_parser | output_processor
    
    return chain

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
# Main execution part
if __name__ == "__main__":
    try:
        # verboseãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
        # Enable verbose mode
        set_verbose(True)
        
        # ãƒã‚§ã‚¤ãƒ³ã‚’ä½œæˆ
        # Create chain
        chain = create_custom_chain()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›
        # User input
        user_input = {"question": "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"}
        
        print("\nğŸ” ãƒã‚§ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™... / Running the chain...\n")
        log_file.write("\nğŸ” ãƒã‚§ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™... / Running the chain...\n\n")
        
        # ãƒã‚§ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
        # Run the chain
        response = chain.invoke(user_input)
        
        print("\nğŸ“ æœ€çµ‚çµæœ / Final Result:")
        print(f"\n{response}\n")
        
        log_file.write("\nğŸ“ æœ€çµ‚çµæœ / Final Result:\n")
        log_file.write(f"\n{response}\n\n")
        
        print(f"\nâœ… è©³ç´°ãªãƒ­ã‚°ã¯ {log_file_path} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        print(f"\nâœ… Detailed logs have been saved to {log_file_path}.")
    
    except Exception as e:
        error_message = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        print(f"\nâŒ {error_message}")
        log_file.write(f"\nâŒ {error_message}\n")
        import traceback
        traceback_str = traceback.format_exc()
        print(traceback_str)
        log_file.write(f"{traceback_str}\n")
    
    finally:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‰ã˜ã‚‹
        # Close the file
        log_file.close()
